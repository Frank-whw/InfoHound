# ğŸ“° InfoHound - 2026å¹´2æœˆ25æ—¥æ˜ŸæœŸä¸‰

> ä»Šæ—¥ç²¾é€‰ 2 ç¯‡æ–‡ç« ï¼Œé¢„è®¡é˜…è¯»æ—¶é—´ 3 åˆ†é’Ÿ

---

## ğŸŒŸ å¤´æ¡

### Show HN: Moonshine Open-Weights STT models â€“ higher accuracy than WhisperLargev3
**æ¥æº**: Hacker News | **è¯„åˆ†**: 8.3/10 ğŸŸ¡

**ä¸ºä»€ä¹ˆé‡è¦**: Moonshine delivers faster, more accurate on-device speech recognition than OpenAI's Whisper while solving critical real-time streaming limitations that make Whisper unsuitable for live voice interfaces.

**ä¸€å¥è¯æ€»ç»“**: Moonshine is an open-source, on-device speech-to-text toolkit optimized for real-time streaming that outperforms Whisper Large v3 in accuracy (6.65% vs 7.44% WER) with 100x lower latency (107ms vs 11,286ms on MacBook Pro) and models as small as 26MB.

**å…³é”®è¦ç‚¹**:
â€¢ Moonshine Medium Streaming achieves 6.65% WER vs Whisper Large v3's 7.44% WER, despite having 6x fewer parameters (245M vs 1.5B) and running 105x faster on MacBook Pro (107ms vs 11,286ms latency)
â€¢ Solves Whisper's fundamental architectural flaws for live speech: fixed 30-second input windows causing wasted computation on zero-padding, and lack of caching forcing redundant processing on repeated calls
â€¢ Supports true streaming with incremental audio addition and cached encoder/decoder state, enabling sub-200ms responsiveness critical for voice interfaces
â€¢ Offers language-specific models (English, Spanish, Mandarin, Japanese, Korean, Vietnamese, Ukrainian, Arabic) that outperform multilingual Whisperâ€”only 33 of 82 Whisper languages achieve sub-20% WER vs Moonshine's focused accuracy
â€¢ Single cross-platform C++ core with OnnxRuntime deploys identically across Python, iOS, Android, macOS, Linux, Windows, Raspberry Pi, and IoT/wearables with native language bindings

**èƒŒæ™¯**: OpenAI's Whisper (2022) democratized open-source speech recognition but was designed for batch transcription, not live applications. The Moonshine team, after hitting Whisper's limitations building real-time voice products, developed purpose-built streaming architectures with flexible input windows and state cachingâ€”achieving both higher accuracy and dramatically lower latency through architectural innovation rather than scale.

**æ ‡ç­¾**: Speech Recognition, Edge AI, Real-time Systems, OnnxRuntime

[é˜…è¯»åŸæ–‡](https://github.com/moonshine-ai/moonshine)


---

## ğŸ”¥ æ·±åº¦æŠ€æœ¯

### ğŸŸ¡ I'm helping my dog vibe code games
**æ¥æº**: Hacker News | **è¯„åˆ†**: 7.1/10

**ä¸ºä»€ä¹ˆé‡è¦**: This demonstrates how creative prompt engineering and automated tooling can turn even random noise into structured software output, pushing the boundaries of what's possible with AI-assisted development.

**è¦ç‚¹**:
â€¢ The core innovation is a carefully crafted prompt that frames random input as 'secret cryptic commands' from an eccentric game designer, combined with strict guardrails (checklist: working audio, WASD controls, visible player, enemies) that improved output quality dramatically
â€¢ The full pipeline: Bluetooth keyboard â†’ Raspberry Pi 5 â†’ Rust proxy (DogKeyboard) that filters dangerous keys â†’ Claude Code â†’ smart treat dispenser triggered by input volume, with games built in 1-2 hours using Godot 4.6 and C#
â€¢ Critical tooling for reliability: Python screenshot capture, automated input sequences for QA testing (Claude played through 6 stages to verify a boss fight), plus linters for Godot scenes and shaders that eliminated cryptic runtime errors

[é˜…è¯»åŸæ–‡](https://www.calebleak.com/posts/dog-game/)


---

## ğŸ“Š ä»Šæ—¥ç»Ÿè®¡

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| æ–‡ç« æ€»æ•° | 2 ç¯‡ |
| å¹³å‡è´¨é‡åˆ† | 7.7/10 |
| é¢„è®¡é˜…è¯»æ—¶é—´ | 3 åˆ†é’Ÿ |

---

*ç”± [InfoHound](https://github.com/Frank-whw/InfoHound) è‡ªåŠ¨ç”Ÿæˆ | AI æ•´ç†ï¼Œäººå·¥é˜…è¯»*
