# ğŸ“° InfoHound - 2026å¹´2æœˆ22æ—¥æ˜ŸæœŸæ—¥

> ä»Šæ—¥ç²¾é€‰ 2 ç¯‡æ–‡ç« ï¼Œé¢„è®¡é˜…è¯»æ—¶é—´ 3 åˆ†é’Ÿ

---

## ğŸŒŸ å¤´æ¡

### Show HN: Llama 3.1 70B on a single RTX 3090 via NVMe-to-GPU bypassing the CPU
**æ¥æº**: Hacker News | **è¯„åˆ†**: 8.1/10 ğŸ”´

**ä¸ºä»€ä¹ˆé‡è¦**: This achieves what was thought impossibleâ€”running a 70B parameter LLM on consumer hardware with 24GB VRAMâ€”by bypassing CPU bottlenecks entirely and streaming layers directly from NVMe to GPU, opening frontier AI to individuals without enterprise infrastructure.

**ä¸€å¥è¯æ€»ç»“**: NTransformer enables Llama 3.1 70B inference on a single RTX 3090 by implementing a 3-tier adaptive caching system that streams model layers via PCIe with optional NVMe direct I/O completely bypassing the CPU, achieving 0.5 tok/s with layer skipping versus 0.006 tok/s for naive mmap.

**å…³é”®è¦ç‚¹**:
â€¢ 83x speedup over baseline: mmap streaming achieved 0.006 tok/s due to page cache thrashing (53GB > 48GB RAM), while tiered caching with layer skip reaches 0.5 tok/sâ€”comparable to cloud API latency for many use cases.
â€¢ PCIe Gen3 x8 is the bottleneck at ~6.5 GB/s: Q4_K_M quantization fits 10 more layers in VRAM (36 vs 26) than Q6_K, reducing tier B transfers and yielding 50% faster inference (0.3 vs 0.2 tok/s) despite lower precision.
â€¢ NVMe direct I/O eliminates CPU entirely: uses userspace VFIO driver with GPU MMIO writes to NVMe controller registers, reading 670MB layers in ~202ms via DMA to pinned GPU-accessible memoryâ€”requires disabling IOMMU and patching NVIDIA DKMS for kernel 6.12+.
â€¢ Layer skip via cosine similarity: skips 20 of 80 layers per token at threshold 0.98 with 'minimal quality loss,' providing 67% speedup (0.3 â†’ 0.5 tok/s) for 70B models without retraining or auxiliary draft models.
â€¢ Zero external dependencies: pure C++/CUDA implementation without PyTorch or cuBLAS, enabling 48.9 tok/s for 8B models resident in VRAMâ€”competitive with optimized frameworks while being radically more deployable.

**èƒŒæ™¯**: Large language models have been gated behind enterprise GPU clusters due to VRAM requirementsâ€”Llama 3.1 70B requires ~140GB for FP16 inference. Prior approaches used CPU offloading or quantization with severe throughput penalties. This work exploits the observation that transformer inference is layer-sequential: only one layer's weights are needed at a time, enabling a 'just-in-time' streaming architecture where weights flow through GPU memory like a pipeline.

**æ ‡ç­¾**: LLM Inference, CUDA, NVMe Direct I/O, Quantization

[é˜…è¯»åŸæ–‡](https://github.com/xaskasdf/ntransformer)


---

## ğŸ”¥ æ·±åº¦æŠ€æœ¯

### ğŸŸ¡ How I use Claude Code: Separation of planning and execution
**æ¥æº**: Hacker News | **è¯„åˆ†**: 8.0/10

**ä¸ºä»€ä¹ˆé‡è¦**: This article presents a battle-tested workflow for AI-assisted coding that solves the #1 failure mode of AI tools: building the wrong thing well through rigorous planning before execution.

**è¦ç‚¹**:
â€¢ The core principle is 'never let Claude write code until you've reviewed and approved a written plan'â€”this separation prevents implementations that work in isolation but break surrounding systems (e.g., ignoring caching layers, duplicating logic, violating ORM conventions).
â€¢ Research phase requires 'deep-read' directives with explicit language ('deeply', 'intricacies') and mandatory written artifacts (research.md) that serve as review surfaces to verify understanding before any planning occurs.
â€¢ The 'Annotation Cycle' is the distinctive value-add: the author reviews plan.md in their editor, adds inline notes correcting assumptions or injecting domain knowledge, and sends Claude back for 1-6 refinement rounds with explicit 'don't implement yet' guards.

[é˜…è¯»åŸæ–‡](https://boristane.com/blog/how-i-use-claude-code/)


---

## ğŸ“Š ä»Šæ—¥ç»Ÿè®¡

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| æ–‡ç« æ€»æ•° | 2 ç¯‡ |
| å¹³å‡è´¨é‡åˆ† | 8.1/10 |
| é¢„è®¡é˜…è¯»æ—¶é—´ | 3 åˆ†é’Ÿ |

---

*ç”± [InfoHound](https://github.com/Frank-whw/InfoHound) è‡ªåŠ¨ç”Ÿæˆ | AI æ•´ç†ï¼Œäººå·¥é˜…è¯»*
